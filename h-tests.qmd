# Hypothesis Testing {#sec-h-tests}

Hypothesis tests allow us to determine whether a finding is “statistically significant,” an idea we briefly encountered in Chapter 3 when discussing how to interpret a regression results table. Testing for statistical significance is the dominant way that the probabilistic conclusions from inferential statistics are communicated in science. However, many statisticians (and other methodologists) have criticized this broad reliance on significance testing as overly simplistic, confusing, or otherwise ill-founded. Given its widespread use, hypothesis testing is important to learn, even though confidence intervals are often a better way to describe statistical results.

As we will see in this chapter, hypothesis testing can often be understood as a particular application of confidence intervals. The risk with hypothesis tests is that we reduce our results down to a binary (significant or not significant), ignoring other nuances that may be present in the results. We will discuss some of these limitations in more detail at the end of the chapter.

## Null and Alternative Hypotheses

The starting point for hypothesis testing is a null hypothesis. Practically speaking, the null hypothesis usually indicates that there is no relationship between variables. Suppose we are studying electoral districts in the US. We might have a binary variable `rural` indicating whether a majority of the district’s residents live in a rural area. A second variable `ideology` is an estimate of the political ideology of the elected official (member of Congress) representing that district. Ideology can be measured on a continuous left-to-right scale (also called “liberal” to “conservative” in the US political system). For analysis of the relationship between these two variables, the null hypothesis would typically be that there is no difference in average ideology between rural and urban districts. Or stated differently, average ideology in rural districts is equal to average ideology in urban districts. The null hypothesis always describes a parameter being estimated, not a sample statistic. For example, a null hypothesis may describe the population mean(s), or a difference that is systematic and not due to the general noisiness that will be present in most samples (something we discuss more in future chapters). Because we are describing parameters, we generally use Greek letters (e.g., to indicate the mean) when writing a null hypothesis as an equation:

$$
H_0: \mu_R - \mu_U = 0
$$ {#eq-rural-null}
We write $H_0$ to represent the null hypothesis, and we are using the subscripts $U$ and $R$ to indicate "urban" and "rural."

Another way to write this same null hypothesis is:

$$
H_0: \mu_R = \mu_U
$$

Every null hypothesis should be paired with an alternative hypothesis, which is normally the logical opposite of the null hypothesis. We write $H_A$ to represent the alternative hypothesis, and our alternative to @eq-rural-null is:

$$
H_0: \mu_R - \mu_U \neq 0
$$ {#eq-rural-alt}

Or equivalently:

$$
H_0: \mu_R \neq \mu_U
$$

One benefit of writing the null and alternative hypotheses in the first form (@eq-rural-null and @eq-rural-alt) is that indicating a difference of means will correspond to how we often set up a regression model. As we saw in @sec-qual-associations, a difference in means can be described with a coefficient in a regression equation. We have already learned that confidence intervals can be constructed for regression coefficients, so let's see if we can evaluate our hypotheses using a confidence interval from a regression.



*Note: The remainder of this chapter still needs to be written. What follows is content adapted from the public domain resource Online Statistics Education: A Multimedia Course of Study (<https://onlinestatbook.com> Project Leader: David M. Lane, Rice University)*

## Comparing More than Two Means with Pairwise Comparisons[^h-tests-1]

[^h-tests-1]: This section is adapted from David M. Lane. “All Pairwise Comparisons Among Means.” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/tests_of_means/pairwise.html>

Many experiments are designed to compare more than two conditions. We will take as an example the case study "Smiles and Leniency."[^h-tests-2] In this study, the effect of different smiles on the leniency shown to a person was investigated. Four different types of smiles (neutral, false, felt, and miserable) were shown. "Type of Smile" is the independent variable, and the dependent variable is a leniency rating given by the subject to a fictional student (depicted with one of the four smiles) in an academic misconduct case. An obvious way to proceed would be to do a t test of the difference between each group mean and each of the other group means. This procedure would lead to the six comparisons shown in @tbl-feltmisneutral.

[^h-tests-2]: <https://onlinestatbook.com/2/case_studies/leniency.html>

+-----------------------+-------------------------------------------+-------------------------------------------+
| false vs. felt        | ![](Images/h-tests-applied/false.png)     | ![](Images/h-tests-applied/felt.png)      |
+-----------------------+-------------------------------------------+-------------------------------------------+
| false vs. miserable   | ![](Images/h-tests-applied/false.png)     | ![](Images/h-tests-applied/miserable.png) |
+-----------------------+-------------------------------------------+-------------------------------------------+
| false vs. neutral     | ![](Images/h-tests-applied/false.png)     | ![](Images/h-tests-applied/neutral.png)   |
+-----------------------+-------------------------------------------+-------------------------------------------+
| felt vs. miserable    | ![](Images/h-tests-applied/felt.png)      | ![](Images/h-tests-applied/miserable.png) |
+-----------------------+-------------------------------------------+-------------------------------------------+
| felt vs. neutral      | ![](Images/h-tests-applied/felt.png)      | ![](Images/h-tests-applied/neutral.png)   |
+-----------------------+-------------------------------------------+-------------------------------------------+
| miserable vs. neutral | ![](Images/h-tests-applied/miserable.png) | ![](Images/h-tests-applied/neutral.png)   |
+-----------------------+-------------------------------------------+-------------------------------------------+

: Six Comparisons among Means. {#tbl-feltmisneutral}

You can certainly conduct a series of six t tests in this manner. However, one potential problem with this approach is that if you did this analysis, you would have six chances to make a Type I error. Therefore, if you were using the 0.05 significance level, the probability that you would make a Type I error on at least one of these comparisons is greater than 0.05.[^h-tests-3] The more means that are compared, the more the Type I error rate is inflated. @fig-pairwisecompfmean shows the number of possible comparisons between pairs of means (pairwise comparisons) as a function of the number of means. If there are only two means, then only one comparison can be made. If there are 12 means, then there are 66 possible comparisons.

[^h-tests-3]: When discussing probability of Type I errors, we assume all null hypotheses are true, since a Type I error can't occur if the null hypothesis is false.

![Number of pairwise comparisons as a function of the number of means.](Images/h-tests-applied/pairwisecompfmean.png){#fig-pairwisecompfmean width="400"}

@fig-type1error shows the probability of a Type I error as a function of the number of means. As you can see, if you have an experiment with 12 means, the probability is about 0.70 that at least one of the 66 comparisons among means would be significant even if all 12 population means were the same.

![Probability of a Type I error as a function of the number of means.](Images/h-tests-applied/type1error.png){#fig-type1error width="400"}

The Type I error rate can be controlled using a test called the Tukey Honestly Significant Difference test or Tukey HSD for short. The Tukey HSD test is one example of a multiple comparison test, but several alternatives are frequently used, such as the Bonferroni correction. Regardless of the exact method used for a multiple comparison test, the interpretation of results is similar. The Tukey HSD is based on a variation of the t distribution that takes into account the number of means being compared. This distribution is called the studentized range distribution.

Normally, statistical software will make all the necessary calculations for you in the background. But to illustrate what sorts of calculations the software is relying on, let's return to the leniency study to see how to compute the Tukey HSD test. You will see that the computations are very similar to those of an independent-groups t test. The steps are outlined below:

1.  Compute the means and variances of each group. For our example, they are shown in @tbl-sum-stats-leniency.

|     | **Condition** |     | **Mean** |     | **Variance** |     |
|:---:|:-------------:|:---:|:--------:|:---:|:------------:|:---:|
|     |     False     |     |   5.37   |     |     3.34     |     |
|     |     Felt      |     |   4.91   |     |     2.83     |     |
|     |   Miserable   |     |   4.91   |     |     2.11     |     |
|     |    Neutral    |     |   4.12   |     |     2.32     |     |

: Means and Variances from the "Smiles and Leniency" Study. {#tbl-sum-stats-leniency}

2.  Compute MSE, which is simply the mean of the variances. It is equal to 2.65.

3.  Compute Q (using the formula below) for each pair of means, where $\bar{X}_i$ is one mean, $\bar{X}_j$ is the other mean, and $n$ is the number of scores in each group. For these data, there are 34 observations per group. The value in the denominator is 0.279. $$ Q=\frac{\bar{X}_i-\bar{X}_j}{\sqrt{\frac{MSE}{n}}} $$

4.  Compute p for each comparison using a Studentized Range Calculator.[^h-tests-4] The degrees of freedom is equal to the total number of observations minus the number of means. For this experiment, df = 136 - 4 = 132.

[^h-tests-4]: <https://onlinestatbook.com/2/calculators/studentized_range_dist.html>

The tests for these data are shown in @tbl-6pairwisecomp.

|     Comparison      | $\bar{X}_i - \bar{X}_j$ | $Q$  |  $p$  |
|:-------------------:|:-----------------------:|:----:|:-----:|
|    False - Felt     |          0.46           | 1.65 | 0.649 |
|  False - Miserable  |          0.46           | 1.65 | 0.649 |
|   False - Neutral   |          1.25           | 4.48 | 0.010 |
|  Felt - Miserable   |          0.00           | 0.00 | 1.000 |
|   Felt - Neutral    |          0.79           | 2.83 | 0.193 |
| Miserable - Neutral |          0.79           | 2.83 | 0.193 |

: Six Pairwise Comparisons. {#tbl-6pairwisecomp}

The only significant comparison is between the false smile and the neutral smile.

It is not unusual to obtain results that on the surface appear paradoxical. For example, these results appear to indicate that (a) the false smile is the same as the miserable smile, (b) the miserable smile is the same as the neutral control, and (c) the false smile is different from the neutral control. This apparent contradiction is avoided if you are careful not to accept the null hypothesis when you fail to reject it. The finding that the false smile is not significantly different from the miserable smile does not mean that they are really the same. Rather it means that there is not convincing evidence that they are different. Similarly, the non-significant difference between the miserable smile and the control does not mean that they are the same. The proper conclusion is that the false smile is higher than the control and that the miserable smile is either (a) equal to the false smile, (b) equal to the control, or (c) somewhere in-between.

The assumptions of the Tukey test are essentially the same as for an independent-groups t test: normality, homogeneity of variance, and independent observations. The test is quite robust to violations of normality. Violating homogeneity of variance can be more problematical than in the two-sample case since the MSE is based on data from all groups. The assumption of independence of observations is important and should not be violated.

## Comparing More than Two Means with ANOVA[^h-tests-5]

[^h-tests-5]: The initial material in this subsection (until the header indicating otherwise) is adapted from David M. Lane. “Introduction.” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/analysis_of_variance/intro.html>

**Analysis of Variance (ANOVA)** is a statistical method used to test differences between two or more means. It may seem odd that the technique is called "Analysis of Variance" rather than "Analysis of Means." The name is appropriate because inferences about means are made by analyzing variance.

ANOVA is used to test general rather than specific differences among means. This can be seen best by example, so we will continue considering the data on leniency and smiles we examined in the prior section on the Tukey HSD test.

ANOVA tests the non-specific null hypothesis that all four population means are equal. That is,

$$ \mu_{false} = \mu_{felt} = \mu_{miserable} = \mu_{neutral} $$

in our example. More generally, the null hypothesis tested by ANOVA is that the population means for all conditions are the same. For whatever data is being examined, this can be written as:

$$ H_0: \mu_1 = \mu_2 = ... = \mu_k $$

where $H_0$ is the null hypothesis and k is the number of conditions (k = 4 in our example).

This non-specific null hypothesis is sometimes called the omnibus null hypothesis. When the omnibus null hypothesis is rejected, the conclusion is that at least one population mean is different from at least one other mean. However, since the ANOVA does not reveal which means are different from which, it offers less specific information than the Tukey HSD test. The Tukey HSD is therefore preferable to ANOVA in this situation.

You might be wondering why you should learn about ANOVA when the Tukey test is better. One reason is that there are complex types of analyses that can be done with ANOVA and not with the Tukey test. A second is that ANOVA is one of the most commonly-used technique for comparing means, and it is important to understand ANOVA in order to understand research reports.

### The Critical Step: Calculating an F Ratio[^h-tests-6] {#sec-the-critical-step-calculating-an-f-ratio}

[^h-tests-6]: This subsection and the following are adapted from David M. Lane. “One-Factor ANOVA (Between Subjects).” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/analysis_of_variance/one-way.html>

There are many types of ANOVA, but for our example, we will use what is called a one-factor between-subjects design. Other types of ANOVA are beyond the scope of what is covered in this text.

More details are beyond the scope of this text and can be found elsewhere,[^h-tests-7] but the critical step in an ANOVA is comparing what is called the mean square error (MSE) to the mean square between (MSB). MSB estimates a larger quantity than MSE only when the population means are not equal, so finding a larger MSB than an MSE is a sign that the population means are not equal. But since MSB could be larger than MSE by chance even if the population means are equal, MSB must be much larger than MSE in order to justify the conclusion that the population means differ. But how much larger must MSB be? For the "Smiles and Leniency" data, the MSB and MSE are 9.179 and 2.649, respectively. Is that difference big enough? To answer, we would need to know the probability of getting that big a difference or a bigger difference if the population means were all equal. The mathematics necessary to answer this question were worked out by the statistician R. Fisher. Although Fisher's original formulation took a slightly different form, the standard method for determining the probability is based on the ratio of MSB to MSE. This ratio is named after Fisher and is called the F ratio.

[^h-tests-7]: For example, see David M. Lane. “One-Factor ANOVA (Between Subjects).” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/analysis_of_variance/one-way.html>

For these data, the F ratio is

$$ F = \frac{9.179}{2.649} = 3.465. $$

Therefore, the MSB is 3.465 times higher than MSE. Would this have been likely to happen if all the population means were equal? That depends on the sample size. With a small sample size, it would not be too surprising because results from small samples are unstable. However, with a very large sample, the MSB and MSE are almost always about the same (assuming the null hypothesis is true), and an F ratio of 3.465 or larger would be very unusual. @fig-fdist shows the sampling distribution of F for the sample size in the "Smiles and Leniency" study. As you can see, it has a positive skew.

![Distribution of F.](Images/h-tests-applied/fdist_smiles.png){#fig-fdist width="375"}

From @fig-fdist, you can see that F ratios of 3.465 or above are unusual occurrences. The area to the right of 3.465 represents the probability of an F that large or larger and is equal to 0.018. In other words, given the null hypothesis that all the population means are equal, the probability value (p) is 0.018 and therefore the null hypothesis can be rejected. The conclusion that at least one of the population means is different from at least one of the others is justified.

The shape of the F distribution depends on the sample size. More precisely, it depends on two degrees of freedom (df) parameters: one for the numerator (MSB) and one for the denominator (MSE). Recall that the degrees of freedom for an estimate of variance is equal to the number of observations minus one. Since the MSB is the variance of k means (where k is the number of groups), it has k - 1 df. The MSE is an average of k variances, each with n - 1 df. Therefore, the df for MSE is k(n - 1) = N - k, where N is the total number of observations, n is the number of observations in each group, and k is the number of groups. To summarize:

$$ df_{\text{numerator}} = k-1 $$$$ df_{\text{denominator}} = N-k $$

For the "Smiles and Leniency" data,

$$ df_{\text{numerator}} = k-1 = 4-1 = 3 $$$$ df_{\text{denominator}} = N-k = 136-4 = 132 $$$$ F = 3.465 $$

An F distribution calculator[^h-tests-8] shows that p = 0.018. Again, because this value is less than 0.05, one would generally reject the null hypothesis and conclude that average leniency varies depending on type of smile. The p-value from an ANOVA is sometimes reported in a larger table of summary results such as @tbl-anova-results-summary.

[^h-tests-8]: <https://onlinestatbook.com/2/calculators/F_dist.html>

|  Source   | df  |   SSQ    |   MS   |   F   |   p    |
|:---------:|:---:|:--------:|:------:|:-----:|:------:|
| Condition |  3  | 27.5349  | 9.1783 | 3.465 | 0.0182 |
|   Error   | 132 | 349.6544 | 2.6489 |       |        |
|   Total   | 135 | 377.1893 |        |       |        |

: ANOVA Summary Table. {#tbl-anova-results-summary}

### Relationship to T Tests and Regression {#sec-relationship-to-t-tests-and-regression}

Since an ANOVA and an independent-groups t test can both test the difference between two means, you might be wondering which one to use. Fortunately, it does not matter since the results will always be the same. When there are only two groups, the following relationship between F and t will always hold:

$$ F(1,dfd) = t^2(df) $$

where dfd is the degrees of freedom for the denominator of the F test and df is the degrees of freedom for the t test. dfd will always equal df. And because of how their probability distributions are constructed, these values of F and t will yield identical p-values for the (two tailed) null hypothesis of no difference between the two means.

There is also a third equivalent way to compare two means: using linear regression, as described in @sec-regression-with-a-qualitative-independent-variable. More generally, linear regression and ANOVA are two sides of the same coin and will yield equivalent results (assuming the same data/assumptions), even when testing for differences among more than two means. Statistical software will generally include a model F statistic among the results shown for a regression, and in the case of a model a single qualitative independent variable, the regression model F statistic will be the same F ratio used in an ANOVA. Because of this equivalence, whether one reports results as an ANOVA or regression is usually a matter of habit and familiarity. In some social science literatures, ANOVA results are rarely reported because researchers typically default to using regression instead.

## Association of Two Qualitative Variables: Chi Square Tests[^h-tests-9] {#sec-contingency-tables}

[^h-tests-9]: This section is adapted from David M. Lane. “Contingency Tables.” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/chi_square/contingency.html>

We previously learned how to describe the relationship between two qualitative variables with a contingency table or bar chart bar chart (@sec-qual-associations). To make a comparison that includes a significance test, we will need to use a distribution called the Chi Square distribution (more details are beyond the scope of this text and can be found elsewhere[^h-tests-10]). As such, our significance test will be called a Chi Square test.

[^h-tests-10]: For example, see David M. Lane. “Chi Square Distribution.” *Online Statistics Education: A Multimedia Course of Study*. <https://onlinestatbook.com/2/chi_square/distribution.html>

To demonstrate the Chi Square test, we again look to data from the Mediterranean Diet and Health case study,[^h-tests-11] in which heart attack survivors were randomly assigned to follow one of two diets. Looking to @tbl-studyfreqs, we want to know whether there is a *significant relationship* between diet and outcome.

[^h-tests-11]: <https://onlinestatbook.com/2/case_studies/diet.html>

+---------------+----------+---------------------+-------------------------+----------+----------+
|               | Outcome  |                     |                         |          | Total    |
+===============+==========+=====================+=========================+==========+==========+
| **Diet**      | Cancers  | Fatal Heart Disease | Non-Fatal Heart Disease | Healthy  |          |
+---------------+----------+---------------------+-------------------------+----------+----------+
| AHA           | 15       | 24                  | 25                      | 239      | 303      |
+---------------+----------+---------------------+-------------------------+----------+----------+
| Mediterranean | 7        | 14                  | 8                       | 273      | 302      |
+---------------+----------+---------------------+-------------------------+----------+----------+
| **Total**     | 22       | 38                  | 33                      | 512      | 605      |
+---------------+----------+---------------------+-------------------------+----------+----------+

: Frequencies for Diet and Health Study. {#tbl-studyfreqs}

As with all other hypothesis tests in this chapter, the null hypothesis indicates no relationship between the two values. And once again, software can calculate a p-value for us to evaluate this hypothesis. But if we are wondering what’s going on under the hood, the first step is to compute the expected frequency for each cell based on the assumption that there is no relationship. These expected frequencies are computed from the totals as follows. We begin by computing the expected frequency for the AHA Diet-Cancers combination. Note that 22/605 subjects developed cancer. The proportion who developed cancer is therefore 0.0364. If there were no relationship between diet and outcome (as the null hypothesis states), then we would expect 0.0364 of those on the AHA diet to develop cancer. Since 303 subjects were on the AHA diet, we would expect (0.0364)(303) = 11.02 cancers on the AHA diet. Similarly, we would expect (0.0364)(302) = 10.98 cancers on the Mediterranean diet. In general, the expected frequency for a cell in the $i$th row and the $j$th column is equal to

$$ E_{ij}=\frac{T_iT_j}{T} $$

where $E_{ij}$ is the expected frequency for cell $i,j$, $T_i$ is the total for the $i$th row, $T_j$ is the total for the $j$th column, and $T$ is the total number of observations. For the AHA Diet-Cancers cell, $i = 1$, $j = 1$, $T_i = 303$, $T_j = 22$, and $T = 605$. @tbl-observedstudyfreqs shows the expected frequencies (in parenthesis) for each cell in the experiment.

The significance test is conducted by computing Chi Square as follows.

$$ \chi^2_3=\sum\frac{(E-O)^2}{E}=16.55 $$

The degrees of freedom is equal to $(r-1)(c-1)$, where $r$ is the number of rows and $c$ is the number of columns. For this example, the degrees of freedom is $(2-1)(4-1) = 3$. A Chi Square calculator[^h-tests-12] can be used to determine that the probability value for a Chi Square of 16.55 with three degrees of freedom is equal to 0.0009. Therefore, the null hypothesis of no relationship between diet and outcome can be rejected.

[^h-tests-12]: <https://onlinestatbook.com/2/calculators/chi_square_prob.html>

+---------------+-------------+---------------------+-------------------------+------------+
|               | **Outcome** |                     |                         |            |
+===============+=============+=====================+=========================+============+
| **Diet**      | Cancers     | Fatal Heart Disease | Non-Fatal Heart Disease | Healthy    |
+---------------+-------------+---------------------+-------------------------+------------+
| AHA           | 15\         | 24\                 | 25\                     | 239\       |
|               | (11.02)     | (19.03)             | (16.53)                 | (256.42)   |
+---------------+-------------+---------------------+-------------------------+------------+
| Mediterranean | 7\          | 14\                 | 8\                      | 273\       |
|               | (10.98)     | (18.97)             | (16.47)                 | (255.58)   |
+---------------+-------------+---------------------+-------------------------+------------+
| **Total**     | 22          | 38                  | 33                      | 512        |
+---------------+-------------+---------------------+-------------------------+------------+

: Observed and Expected Frequencies for Diet and Health Study. {#tbl-observedstudyfreqs}

### Drawing Substantive Conclusions from a Contingency Table[^h-tests-13] {#sec-drawing-substantive-conclusions-from-a-contingency-table}

[^h-tests-13]: This subsection is written by Nathan Favero.

Now that we know there is a statistically significant relationship between diet and health outcome (since we rejected the null hypothesis of no relationship), we will naturally wonder what kind of relationship exists. Specifically, which diet is associated with better health? To answer this, we must look to the specific values within the cells and describe the patterns we observe. We already covered how to do this in @sec-qual-associations when percentages are provided by column or row. An even more straightforward way to interpret the association is to make note of where the observed frequency differs notably from the expected frequency.

From @tbl-observedstudyfreqs, we can see that for those on the AHA diet, the frequencies for cancers, fatal heart disease, and non-fatal heart disease are all higher than expected. At the same time, the frequency for "healthy" is lower than expected on the AHA diet. Results for the Mediterranean diet are the exact opposite: cancers and both types of heart disease occur less frequently than expected, and the healthy outcome occurs more than expected. Thus, the Mediterranean diet is unambiguously associated with better outcomes than the AHA diet.

Though it is easy to make sense of results in this manner based on a comparison of observe to expected frequencies, the typical contingency table you encounter will probably not display expected frequencies. Instead, it is common to include percentages by row or by column, which is why we focused on interpreting such tables in @sec-describing-the-relationship-between-two-qualitative-variables.

The good news is that regardless of whether we examine (1) expected versus observed outcomes, (2) percentages by row, or (3) percentages by column, we reach the same conclusion: health outcomes are uniformly better for those on the Mediterranean diet. All three approaches are equally valid ways of evaluating associations from a contingency table, and you normally need use only one. We briefly mention all three here for learning purposes since each approach is one you may encounter in your own analysis or in a research report.
